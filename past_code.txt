import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from pyspark.sql import Window
from pyspark.sql import functions as F
from pyspark.sql.functions import coalesce

st.set_page_config(page_title="Elasticity Price Monitor", layout="wide")

# ---CONFIG---
CSV_SERIES = "prediction_input_series.csv"
CSV_ELAST = "final_pe_mt.csv"

YEARS_TRAIN = (2021, 2023)
YEARS_VAL = (2024, 2025)
eps = 1e-9

@st.cache_data(show_spinner=True)
def load_series(path_series=CSV_SERIES):
    df_series = pd.read_csv(path_series, low_memory=False)

    def pick_col(df, opciones):
        for c in opciones:
            if c in df.columns:
                return df[c]
        return pd.Series([np.nan]*len(df), index=df.index)
    
    # Normaliza fechas
    if "PERIOD" in df_series.columns:
        s = df_series["PERIOD"].astype(str).str.strip()
        periods = pd.to_datetime(s, format="%Y_%m", errors="coerce")
        if periods.isna().any():
            m1 = s.str.match(r"^\d{4}-\d{1,2}$")
            periods.loc[m1] = pd.to_datetime(s[m1] + "-01", errors="coerce")
            m2 = s.str.match(r"^\d{6}$")
            periods.loc[m2] = pd.to_datetime(s[m2], format="%Y%m", errors="coerce")
            m3 = s.str.match(r"^\d{8}$")
            periods.loc[m3] = pd.to_datetime(s[m3], format="%Y%m%d", errors="coerce")
        df_series["PERIOD"] = periods
    else:
        y = pick_col(df_series,["YEAR", "year"])
        m = pick_col(df_series,["MONTH", "month"])
        y_str = y.astype("Int64").astype(str).zfill(4)
        m_str = m.astype("Int64").astype(str).zfill(2)
        df_series["PERIOD"] = pd.to_datetime(y_str + "-" + m_str + "-01", errors="coerce")

    # Estandarización de nombres
    rename_map = {}
    if "CATEGORY_REVISED" in df_series.columns:
        rename_map["CATEGORY_REVISED"] = "CATEGORIA"
    if "SUBCATEGORY_REVISED" in df_series.columns:
        rename_map["SUBCATEGORY_REVISED"] = "SUBCATEGORIA"
    if "SKU_GROUP" in df_series.columns:
        rename_map["SKU_GROUP"] = "SKU"
    if "UC_CALCULATED" in df_series.columns:
        rename_map["UC_CALCULATED"] = "UC"
    if "INGRESO_NETO" in df_series.columns:
        rename_map["INGRESO_NETO"] = "INGRESO"
    if "PRICE" in df_series.columns:
        rename_map["PRICE"] = "PRECIO"

    if rename_map:
        df_series = df_series.rename(columns=rename_map)

    for need in ["PERIOD", "SKU"]:
        if need not in df_series.columns:
            raise KeyError(f"Missing column {need} in series")
    
    for col in ["UC","INGRESO"]:
        if col in df_series.columns:
            df_series[col] = pd.to_numeric(df_series[col], errors='coerce').replace([np.inf, -np.inf], np.nan)
    return df_series

# Carga de datos
df = load_series()

# Sidebar
st.sidebar.header('Filtros')
metric = st.sidebar.radio(
    "Métrica",
    ("Volumen (UC)", "Ventas ($ Ingreso)"),
    help="UC = Unit Cases Estandarizados; Ingreso = Ventas Monetizadas"
)
metric_col = "UC" if metric.startswith("Volumen") else "INGRESO"
if metric_col not in df.columns:
    st.warning(f"No encontré la columna '{metric_col}'. Intento usar otra disponible.")
    metric_col = "UC" if "UC" in df.columns else ("INGRESO" if "INGRESO" in df.columns else None)
if metric_col is None:
    st.error("No hay columnas de métricas disponibles (UC/INGRESO).")
    st.stop()

# Filtros jerárquicos
cats = ["All"] + sorted(df.get("CATEGORIA", pd.Series(dtype=str)).dropna().unique().tolist())
cat_sel = st.sidebar.selectbox("Categoría", cats)

if cat_sel != "All" and "SUBCATEGORIA" in df.columns:
    subs = ["All"] + sorted(df.loc[df["CATEGORIA"]==cat_sel,"SUBCATEGORIA"].dropna().unique().tolist())
else:
    subs = ["All"] + sorted(df.get("SUBCATEGORIA", pd.Series(dtype=str)).dropna().unique().tolist())
sub_sel = st.sidebar.selectbox("Subcategoría", subs)

if sub_sel != "All" and "SUBCATEGORIA" in df.columns:
    skus = ['All'] + sorted(df.loc[df["SUBCATEGORIA"]==sub_sel,"SKU"].dropna().unique().tolist())
else:
    skus = ['All'] + sorted(df.get("SKU").dropna().unique().tolist())
sku_sel = st.sidebar.selectbox("SKU", skus)

# Fechas
periods = pd.to_datetime(df["PERIOD"], errors="coerce")
min_d = periods.min()
max_d = periods.max()
if pd.isna(min_d) or pd.isna(max_d):
    st.warning("No hay fechas válidas en 'PERIOD'. Uso un rango por defecto.")
    min_d = pd.Timestamp("2021-01-01")
    max_d = pd.Timestamp.today().normalize()
default_range = [min_d.date(), max_d.date()]
dr = st.sidebar.date_input("Rango de Fechas", value=default_range, key="date_range")
if isinstance(dr, (list, tuple)) and len(dr) == 2:
    d_start, d_end = pd.Timestamp(dr[0]), pd.Timestamp(dr[1])
else:
    d_start = pd.Timestamp(dr); d_end = pd.Timestamp(dr)

# Nivel
level = st.sidebar.radio("Nivel", ["Total","Categoría","Subcategoría","SKU"], horizontal=True)

# Tabs ARRIBA
tab1, tab2 = st.tabs(["Sales Explorer", "Elasticity Performance"])

# ===================== TAB 1 =====================
with tab1:
    # Filtros base
    d = df.copy()
    if cat_sel != "All" and "CATEGORIA" in d.columns:
        d = d[d["CATEGORIA"] == cat_sel]
    if sub_sel != "All" and "SUBCATEGORIA" in d.columns:
        d = d[d["SUBCATEGORIA"] == sub_sel]
    if sku_sel != "All" and "SKU" in d.columns:
        d = d[d["SKU"] == sku_sel]
    d = d[(d["PERIOD"] >= d_start) & (d["PERIOD"] <= d_end)]

    # KPIs (sin 'Registros')
    col1, col2 = st.columns(2)
    total_metric = pd.to_numeric(d[metric_col], errors="coerce").sum(skipna=True)
    n_skus = d["SKU"].nunique()
    col1.metric(f"Total {metric}", f"{total_metric:,.0f}")
    col2.metric("SKU's", f"{n_skus:,}")

    # Agregado
    group = ["PERIOD"]; color = None
    if level=="Categoría" and "CATEGORIA" in d.columns:
        group.append("CATEGORIA"); color = "CATEGORIA"
    elif level=="Subcategoría" and "SUBCATEGORIA" in d.columns:
        group.append("SUBCATEGORIA"); color = "SUBCATEGORIA"
    elif level=="SKU" and "SKU" in d.columns:
        group.append("SKU"); color = "SKU"

    agg = d.groupby(group, dropna=False)[metric_col].sum().reset_index()

    # Gráfico
    title = f"{level} Mensual {('UC' if metric_col=='UC' else 'Ingreso')}"
    fig = px.line(agg, x="PERIOD", y=metric_col, color=color, title=title)
    fig.update_layout(hovermode="x unified", legend_title=None)
    st.plotly_chart(fig, use_container_width=True)

    # Botón de descarga (sin mostrar la tabla)
    csv = agg.to_csv(index=False).encode("utf-8")
    st.download_button("Descargar CSV", csv, file_name=f"{level}_{metric_col}.csv", mime="text/csv")

# ===================== TAB 2 =====================
def load_joined_for_elasticity():
    pe = pd.read_csv(CSV_ELAST, low_memory=False)
    pe = pe.rename(columns={
        "SKU_NAME": "SKU",
        "FINAL_PE": "FINAL_PE",
        "PE_RANGE_CHECK_OVERALL": "PE_RANGE_CHECK_OVERALL",
        "P1_MIN": "P1_MIN",
        "P1_MAX": "P1_MAX",
        "P2_MIN": "P2_MIN",
        "P2_MAX": "P2_MAX",
        "FINAL_PE_SOURCE": "FINAL_PE_SOURCE",
    })
    dj = df.merge(pe, on="SKU", how="left")
    dj = dj.sort_values(["SKU", "PERIOD"]).copy()

    price_col = next((c for c in ["PRECIO", "PRICE"] if c in dj.columns), None)
    uc_col    = "UC" if "UC" in dj.columns else None
    missing = []
    if price_col is None: missing.append("PRECIO/PRICE")
    if uc_col is None: missing.append("UC")
    if missing:
        raise KeyError(f"Faltan columnas requeridas: {', '.join(missing)}")

    dj[price_col] = pd.to_numeric(dj[price_col], errors="coerce")
    dj[uc_col]    = pd.to_numeric(dj[uc_col], errors="coerce")

    dj["lag_price"] = dj.groupby("SKU")[price_col].shift(1)
    dj["lag_uc"]    = dj.groupby("SKU")[uc_col].shift(1)

    dj["dln_precio"] = np.nan
    ok_p = (dj[price_col] > 0) & (dj["lag_price"] > 0)
    dj.loc[ok_p, "dln_precio"] = np.log(dj.loc[ok_p, price_col]) - np.log(dj.loc[ok_p, "lag_price"])

    dj["dln_uc"] = np.nan
    ok_u = (dj[uc_col] > 0) & (dj["lag_uc"] > 0)
    dj.loc[ok_u, "dln_uc"] = np.log(dj.loc[ok_u, uc_col]) - np.log(dj.loc[ok_u, "lag_uc"])

    dj["FINAL_PE"] = pd.to_numeric(dj.get("FINAL_PE"), errors="coerce")
    dj["impacto_esperado"] = dj["FINAL_PE"] * dj["dln_precio"]
    return dj

def mape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred)) & (np.abs(y_true) > eps)
    if mask.sum()==0: return np.nan
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + eps))) * 100

def smape(y_true, y_pred, eps=1e-8):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred))
    if mask.sum()==0: return np.nan
    num = np.abs(y_true[mask]-y_pred[mask])
    den = (np.abs(y_true[mask]) + np.abs(y_pred[mask]) + eps)
    return np.mean(2*num/den) * 100

def diracc(y_true, y_pred):
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    mask = (~np.isnan(y_true)) & (~np.isnan(y_pred))
    if mask.sum()==0: return np.nan
    return (np.sign(y_true[mask])==np.sign(y_pred[mask])).mean() * 100

def apply_ranges_block(dd, apply_to="Esperado"):
    d = dd.copy()
    for c in ["P1_MIN","P1_MAX","P2_MIN","P2_MAX","PE_RANGE_CHECK_OVERALL"]:
        if c in d.columns:
            d[c] = pd.to_numeric(d[c], errors="coerce")
    cond1 = (d["PE_RANGE_CHECK_OVERALL"] == 1)
    cond2 = (d["PE_RANGE_CHECK_OVERALL"] == 2)

    e_min = np.where(cond1, d["P1_MIN"], np.where(cond2, d["P2_MIN"], np.nan))
    e_max = np.where(cond1, d["P1_MAX"], np.where(cond2, d["P2_MAX"], np.nan))

    e_teor = d["dln_uc"] / d["dln_precio"]
    e_teor = e_teor.replace([np.inf, -np.inf], np.nan)

    eps_local = 1e-8
    no_move = d["dln_precio"].abs() < eps_local

    lower = np.where(np.isnan(e_min), -np.inf, e_min)
    upper = np.where(np.isnan(e_max),  np.inf,  e_max)
    e_used = np.clip(e_teor.to_numpy(), lower, upper)

    d["real_clipped"] = e_used * d["dln_precio"].to_numpy()
    d.loc[no_move, "real_clipped"] = 0.0

    d["clip_source"] = np.nan
    to_min = (e_teor.to_numpy() < lower) & ~no_move.to_numpy()
    to_max = (e_teor.to_numpy() > upper) & ~no_move.to_numpy()
    d.loc[to_min & cond1.to_numpy(), "clip_source"] = "P1_MIN"
    d.loc[to_max & cond1.to_numpy(), "clip_source"] = "P1_MAX"
    d.loc[to_min & cond2.to_numpy(), "clip_source"] = "P2_MIN"
    d.loc[to_max & cond2.to_numpy(), "clip_source"] = "P2_MAX"
    d.loc[no_move, "clip_source"] = "NO_PRICE_MOVE"

    d["elasticidad_usada_real"] = e_used
    return d

def clip_real_with_ranges(dsku):
    return apply_ranges_block(dsku, apply_to="Esperado")

with tab2:
    st.subheader("Elasticidad vs. Real — clipping en Real con P1/P2")

    dj = load_joined_for_elasticity()

    # Filtros coherentes con la barra lateral
    if cat_sel != "All" and "CATEGORIA" in dj.columns:
        dj = dj[dj["CATEGORIA"] == cat_sel]
    if sub_sel != "All" and "SUBCATEGORIA" in dj.columns:
        dj = dj[dj["SUBCATEGORIA"] == sub_sel]

    # SKU
    sku_pick = sku_sel if sku_sel != "All" else st.selectbox(
        "Selecciona SKU", sorted(dj["SKU"].dropna().unique())
    )

    dsku = dj[dj["SKU"] == sku_pick].copy()
    dsku = clip_real_with_ranges(dsku)

    # Particiones de tiempo
    d_train = dsku[dsku["PERIOD"].dt.year.between(YEARS_TRAIN[0], YEARS_TRAIN[1])]
    d_val   = dsku[dsku["PERIOD"].dt.year.between(YEARS_VAL[0], YEARS_VAL[1])]
    d_all   = dsku[dsku["PERIOD"].dt.year.between(YEARS_TRAIN[0], YEARS_VAL[1])]

    # Vista
    vista = st.radio(
        "Vista",
        ["Real vs. Impacto Esperado", "Real Ajustado vs. Impacto Esperado", "Comparar Ambos"],
        horizontal=True
    )

    # Series
    real_tr_raw = d_train["dln_uc"]; real_val_raw = d_val["dln_uc"]; real_all_raw = d_all["dln_uc"]
    real_tr = d_train["real_clipped"]; real_val = d_val["real_clipped"]; real_all = d_all["real_clipped"]
    exp_tr = d_train["impacto_esperado"]; exp_val = d_val["impacto_esperado"]; exp_all = d_all["impacto_esperado"]

    if vista == "Real vs. Impacto Esperado":
        real_tr, real_val, real_all = real_tr_raw, real_val_raw, real_all_raw

    # Métricas (solo Train y Val para no duplicar)
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("MAPE Train (↓ mejor)", f"{mape(real_tr, exp_tr):.2f}%")
    with c2:
        st.metric("SMAPE Train (↓ mejor)", f"{smape(real_tr, exp_tr):.2f}%")
    with c3:
        st.metric("DIR Train (↑ mejor)", f"{diracc(real_tr, exp_tr):.2f}%")

    c4, c5, c6 = st.columns(3)
    with c4:
        st.metric("MAPE Val (↓ mejor)", f"{mape(real_val, exp_val):.2f}%")
    with c5:
        st.metric("SMAPE Val (↓ mejor)", f"{smape(real_val, exp_val):.2f}%")
    with c6:
        st.metric("DIR Val (↑ mejor)", f"{diracc(real_val, exp_val):.2f}%")

    # Gráfico único
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=d_all["PERIOD"], y=np.expm1(real_all)*100,
        mode="lines", name=("Cambio Real (%)" if vista=="Real vs. Impacto Esperado" else "Cambio Real Ajustado (%)"),
        line=dict(width=2)
    ))
    if vista == "Comparar Ambos":
        fig.add_trace(go.Scatter(
            x=d_all["PERIOD"], y=np.expm1(d_all["dln_uc"])*100,
            mode="lines", name="Cambio Real (%)", line=dict(width=2, dash='dot')
        ))
    fig.add_trace(go.Scatter(
        x=d_train["PERIOD"], y=np.expm1(exp_tr)*100,
        mode="lines", name="Impacto Esperado (Train) (%)", line=dict(width=2, dash='dash')
    ))
    fig.add_trace(go.Scatter(
        x=d_val["PERIOD"], y=np.expm1(exp_val)*100,
        mode="lines", name="Impacto Esperado (Val) (%)", line=dict(width=2, dash='dash')
    ))

    fig.add_vline(x=pd.to_datetime("2023-12-31"), line_dash='dash')
    fig.update_layout(
        hovermode="x unified",
        xaxis_title="Periodo",
        yaxis_title="Cambio (%)",
        height=550
    )
    st.plotly_chart(fig, use_container_width=True)

    # (Se eliminó la tabla de filas con ajuste aplicado)
